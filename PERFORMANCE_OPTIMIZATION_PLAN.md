# üöÄ Performance Optimization Plan

## Phase 1: Database & Connection Optimization ‚úÖ COMPLETED

### What We've Implemented:
1. **Connection Pooling**: Added `psycopg2.pool.ThreadedConnectionPool` to reuse database connections
2. **Context Manager**: Created `get_db_connection()` for automatic connection management
3. **Caching Layer**: Implemented `@simple_cache` decorator with TTL for frequently accessed data
4. **Performance Logging**: Added execution time monitoring for critical functions

### Results Expected:
- **60-80% reduction** in database connection overhead
- **50-70% faster** repeated data access through caching
- **Better resource management** and reduced connection leaks

---

## Phase 2: Critical Function Optimization ‚úÖ COMPLETED

### Optimized Functions:
1. `get_gmm_name()` - 10 min cache
2. `get_all_columns_data_all_compounds()` - 15 min cache (most expensive)
3. `get_column_names()` - 10 min cache
4. `get_metabolite_data()` - 5 min cache

### Next Priority Functions to Optimize:

```python
# High Impact Functions (implement next):
- get_multiple_bacteria_top_metabolites()
- get_bacteria_data()
- get_top_bottom_bacteria_values()
- get_gmm_name_by_type()
```

---

## Phase 3: Callback Performance (IN PROGRESS)

### What's Needed:
1. **Prevent Unnecessary Updates**:
```python
@app.callback(
    prevent_initial_call=True,  # ‚úÖ Added
    suppress_callback_exceptions=True
)
```

2. **Batch Multiple Outputs**:
```python
# Instead of separate callbacks, combine related outputs
@app.callback(
    [Output('plot1', 'figure'), Output('plot2', 'figure')],
    [Input('dropdown', 'value')]
)
```

3. **Add Loading States**: ‚ö†Ô∏è PRIORITY
```python
# Add to layout.py loading wrappers
dcc.Loading(
    type="cube",  # Better visual feedback
    children=[your_component]
)
```

---

## Phase 4: Data Preprocessing (RECOMMENDED NEXT)

### Create Materialized Views:
```sql
-- Pre-aggregate common queries
CREATE MATERIALIZED VIEW top_metabolites_by_bacteria AS
SELECT 
    bacteria,
    metabolite,
    value,
    RANK() OVER (PARTITION BY metabolite ORDER BY value DESC) as rank
FROM unpivoted_data;

-- Refresh periodically
REFRESH MATERIALIZED VIEW top_metabolites_by_bacteria;
```

### Pre-calculate Heavy Operations:
```python
# Create startup script to pre-cache common data
def warm_cache():
    tables = ['gmm_test_1', 'in_vivo']
    for table in tables:
        get_gmm_name(table)  # Cache all metabolite names
        get_column_names(table)  # Cache all bacteria names
```

---

## Phase 5: Advanced Optimizations

### 1. Async Operations (Future):
```python
import asyncio
import asyncpg

async def fetch_data_async(query):
    # Non-blocking database operations
    pass
```

### 2. Redis Caching Layer:
```python
import redis
import pickle

class RedisCache:
    def __init__(self):
        self.redis_client = redis.Redis(host='localhost', port=6379, db=0)
    
    def get(self, key):
        data = self.redis_client.get(key)
        return pickle.loads(data) if data else None
    
    def set(self, key, value, ttl=300):
        self.redis_client.setex(key, ttl, pickle.dumps(value))
```

### 3. DataFrame Optimization:
```python
# Use more efficient data types
def optimize_dataframe(df):
    for col in df.select_dtypes(include=['float64']):
        df[col] = pd.to_numeric(df[col], downcast='float')
    for col in df.select_dtypes(include=['int64']):
        df[col] = pd.to_numeric(df[col], downcast='integer')
    return df
```

---

## Implementation Priority

### üî• IMMEDIATE (This Week):
1. ‚úÖ Database connection pooling
2. ‚úÖ Basic caching for expensive functions
3. ‚ö†Ô∏è **Optimize remaining heavy functions**
4. ‚ö†Ô∏è **Add better loading states**

### üìà SHORT TERM (Next 2 Weeks):
1. Pre-aggregate common database queries
2. Implement data preprocessing on startup
3. Add callback performance monitoring
4. Optimize DataFrame operations

### üöÄ LONG TERM (Next Month):
1. Consider Redis for distributed caching
2. Implement async database operations
3. Add progressive loading for large datasets
4. Consider CDN for static assets

---

## Monitoring & Metrics

### Track These KPIs:
```python
# Add to your callbacks
import time
start_time = time.time()
# ... your code ...
execution_time = time.time() - start_time
logging.info(f"Callback executed in {execution_time:.2f}s")
```

### Target Performance:
- **Initial load**: < 3 seconds
- **Plot rendering**: < 1 second
- **Data filtering**: < 500ms
- **Cache hit ratio**: > 70%

---

## Quick Wins to Implement Now

### 1. Add to `app.py`:
```python
# Initialize connection pool on startup
from compare_tumor.data_functions import db_pool
db_pool.initialize_pool(minconn=5, maxconn=20)

# Warm cache on startup
def warm_cache():
    get_gmm_name('gmm_test_1')
    get_gmm_name('in_vivo')
    get_column_names('gmm_test_1')
    get_column_names('in_vivo')

warm_cache()
```

### 2. Add Better Error Handling:
```python
def safe_callback(func):
    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        try:
            return func(*args, **kwargs)
        except Exception as e:
            logging.error(f"Callback error: {e}")
            return create_empty_figure("Error", str(e))
    return wrapper
```

### 3. Optimize Plot Creation:
```python
# Use more efficient plot updates
def update_figure_efficiently(fig, new_data):
    # Instead of recreating, update existing traces
    fig.data[0].x = new_data['x']
    fig.data[0].y = new_data['y']
    return fig
```

## Expected Performance Improvements

After implementing Phase 1-2:
- **Database queries**: 60-80% faster
- **Repeated operations**: 70-90% faster  
- **Memory usage**: 30-50% reduction
- **User experience**: Significantly more responsive

## Next Steps

1. **Test current optimizations** - verify performance gains
2. **Implement remaining function caching** - focus on `get_bacteria_data()`
3. **Add comprehensive loading states** - improve UX during processing
4. **Monitor performance metrics** - track improvements
5. **Plan Phase 4 implementation** - data preprocessing strategy 